{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import namedtuple\n",
    "import torchvision\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms as T\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from math import log10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPn133poWDUR"
   },
   "source": [
    "# VQVAE Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T12:47:49.565134Z",
     "iopub.status.busy": "2025-09-26T12:47:49.564794Z",
     "iopub.status.idle": "2025-09-26T12:47:49.605871Z",
     "shell.execute_reply": "2025-09-26T12:47:49.605076Z",
     "shell.execute_reply.started": "2025-09-26T12:47:49.565106Z"
    },
    "id": "C_F8_aWIWDUT",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Down conv block with attention.\n",
    "    Sequence of following block\n",
    "    1. Resnet block with time embedding\n",
    "    2. Attention block\n",
    "    3. Downsample\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim,\n",
    "                 down_sample, num_heads, num_layers, attn, norm_channels, cross_attn=False, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.down_sample = down_sample\n",
    "        self.attn = attn\n",
    "        self.context_dim = context_dim\n",
    "        self.cross_attn = cross_attn\n",
    "        self.t_emb_dim = t_emb_dim\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        if self.t_emb_dim is not None:\n",
    "            self.t_emb_layers = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(self.t_emb_dim, out_channels)\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ])\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if self.attn:\n",
    "            self.attention_norms = nn.ModuleList(\n",
    "                [nn.GroupNorm(norm_channels, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "\n",
    "            self.attentions = nn.ModuleList(\n",
    "                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "\n",
    "        if self.cross_attn:\n",
    "            assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n",
    "            self.cross_attention_norms = nn.ModuleList(\n",
    "                [nn.GroupNorm(norm_channels, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            self.cross_attentions = nn.ModuleList(\n",
    "                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            self.context_proj = nn.ModuleList(\n",
    "                [nn.Linear(context_dim, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.down_sample_conv = nn.Conv2d(out_channels, out_channels,\n",
    "                                          4, 2, 1) if self.down_sample else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t_emb=None, context=None):\n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            # Resnet block of Unet\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            if self.t_emb_dim is not None:\n",
    "                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "\n",
    "            if self.attn:\n",
    "                # Attention block of Unet\n",
    "                batch_size, channels, h, w = out.shape\n",
    "                in_attn = out.reshape(batch_size, channels, h * w)\n",
    "                in_attn = self.attention_norms[i](in_attn)\n",
    "                in_attn = in_attn.transpose(1, 2)\n",
    "                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "                out = out + out_attn\n",
    "\n",
    "            if self.cross_attn:\n",
    "                assert context is not None, \"context cannot be None if cross attention layers are used\"\n",
    "                batch_size, channels, h, w = out.shape\n",
    "                in_attn = out.reshape(batch_size, channels, h * w)\n",
    "                in_attn = self.cross_attention_norms[i](in_attn)\n",
    "                in_attn = in_attn.transpose(1, 2)\n",
    "                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim\n",
    "                context_proj = self.context_proj[i](context)\n",
    "                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n",
    "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "                out = out + out_attn\n",
    "\n",
    "        # Downsample\n",
    "        out = self.down_sample_conv(out)\n",
    "        return out\n",
    "\n",
    "class MidBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Mid conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Resnet block with time embedding\n",
    "    2. Attention block\n",
    "    3. Resnet block with time embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads, num_layers, norm_channels, cross_attn=None, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.t_emb_dim = t_emb_dim\n",
    "        self.context_dim = context_dim\n",
    "        self.cross_attn = cross_attn\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if self.t_emb_dim is not None:\n",
    "            self.t_emb_layers = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(t_emb_dim, out_channels)\n",
    "                )\n",
    "                for _ in range(num_layers + 1)\n",
    "            ])\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [nn.GroupNorm(norm_channels, out_channels)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.attentions = nn.ModuleList(\n",
    "            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "        if self.cross_attn:\n",
    "            assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n",
    "            self.cross_attention_norms = nn.ModuleList(\n",
    "                [nn.GroupNorm(norm_channels, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            self.cross_attentions = nn.ModuleList(\n",
    "                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            self.context_proj = nn.ModuleList(\n",
    "                [nn.Linear(context_dim, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t_emb=None, context=None):\n",
    "        out = x\n",
    "\n",
    "        # First resnet block\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first[0](out)\n",
    "        if self.t_emb_dim is not None:\n",
    "            out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n",
    "        out = self.resnet_conv_second[0](out)\n",
    "        out = out + self.residual_input_conv[0](resnet_input)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            # Attention Block\n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "\n",
    "            if self.cross_attn:\n",
    "                assert context is not None, \"context cannot be None if cross attention layers are used\"\n",
    "                batch_size, channels, h, w = out.shape\n",
    "                in_attn = out.reshape(batch_size, channels, h * w)\n",
    "                in_attn = self.cross_attention_norms[i](in_attn)\n",
    "                in_attn = in_attn.transpose(1, 2)\n",
    "                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim\n",
    "                context_proj = self.context_proj[i](context)\n",
    "                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n",
    "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "                out = out + out_attn\n",
    "\n",
    "\n",
    "            # Resnet Block\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i + 1](out)\n",
    "            if self.t_emb_dim is not None:\n",
    "                out = out + self.t_emb_layers[i + 1](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i + 1](out)\n",
    "            out = out + self.residual_input_conv[i + 1](resnet_input)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Up conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Upsample\n",
    "    1. Concatenate Down block output\n",
    "    2. Resnet block with time embedding\n",
    "    3. Attention Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim,\n",
    "                 up_sample, num_heads, num_layers, attn, norm_channels):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.up_sample = up_sample\n",
    "        self.t_emb_dim = t_emb_dim\n",
    "        self.attn = attn\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if self.t_emb_dim is not None:\n",
    "            self.t_emb_layers = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(t_emb_dim, out_channels)\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ])\n",
    "\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        if self.attn:\n",
    "            self.attention_norms = nn.ModuleList(\n",
    "                [\n",
    "                    nn.GroupNorm(norm_channels, out_channels)\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            self.attentions = nn.ModuleList(\n",
    "                [\n",
    "                    nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.up_sample_conv = nn.ConvTranspose2d(in_channels, in_channels,\n",
    "                                                 4, 2, 1) \\\n",
    "            if self.up_sample else nn.Identity()\n",
    "\n",
    "    def forward(self, x, out_down=None, t_emb=None):\n",
    "        # Upsample\n",
    "        x = self.up_sample_conv(x)\n",
    "\n",
    "        # Concat with Downblock output\n",
    "        if out_down is not None:\n",
    "            x = torch.cat([x, out_down], dim=1)\n",
    "\n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            # Resnet Block\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            if self.t_emb_dim is not None:\n",
    "                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "\n",
    "            # Self Attention\n",
    "            if self.attn:\n",
    "                batch_size, channels, h, w = out.shape\n",
    "                in_attn = out.reshape(batch_size, channels, h * w)\n",
    "                in_attn = self.attention_norms[i](in_attn)\n",
    "                in_attn = in_attn.transpose(1, 2)\n",
    "                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "                out = out + out_attn\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CyFTKi_WDUW"
   },
   "source": [
    "# VQVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T12:47:49.607746Z",
     "iopub.status.busy": "2025-09-26T12:47:49.607510Z",
     "iopub.status.idle": "2025-09-26T12:47:49.635520Z",
     "shell.execute_reply": "2025-09-26T12:47:49.634756Z",
     "shell.execute_reply.started": "2025-09-26T12:47:49.607730Z"
    },
    "id": "QWFbgaP9WDUX",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.im_channels = 3\n",
    "        self.down_channels = [64, 128, 256, 256]\n",
    "        self.mid_channels = [256, 256]\n",
    "        self.down_sample = [True, True, True]\n",
    "        self.num_down_layers = 2\n",
    "        self.num_mid_layers = 2\n",
    "        self.num_up_layers = 2\n",
    "\n",
    "        self.attns = [False, False, False]\n",
    "\n",
    "        # Latent Dimension\n",
    "        self.z_channels = 4\n",
    "        self.codebook_size = 8192\n",
    "        self.norm_channels = 32\n",
    "        self.num_heads = 4\n",
    "\n",
    "        # Assertion to validate the channel information\n",
    "        assert self.mid_channels[0] == self.down_channels[-1]\n",
    "        assert self.mid_channels[-1] == self.down_channels[-1]\n",
    "        assert len(self.down_sample) == len(self.down_channels) - 1\n",
    "        assert len(self.attns) == len(self.down_channels) - 1\n",
    "\n",
    "        self.up_sample = list(reversed(self.down_sample))\n",
    "\n",
    "        ##################### Encoder ######################\n",
    "        self.encoder_conv_in = nn.Conv2d(self.im_channels, self.down_channels[0], kernel_size=3, padding=(1, 1))\n",
    "\n",
    "        # Downblock + Midblock\n",
    "        self.encoder_layers = nn.ModuleList([])\n",
    "        for i in range(len(self.down_channels) - 1):\n",
    "            self.encoder_layers.append(DownBlock(self.down_channels[i], self.down_channels[i + 1],\n",
    "                                                 t_emb_dim=None, down_sample=self.down_sample[i],\n",
    "                                                 num_heads=self.num_heads,\n",
    "                                                 num_layers=self.num_down_layers,\n",
    "                                                 attn=self.attns[i],\n",
    "                                                 norm_channels=self.norm_channels))\n",
    "\n",
    "        self.encoder_mids = nn.ModuleList([])\n",
    "        for i in range(len(self.mid_channels) - 1):\n",
    "            self.encoder_mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i + 1],\n",
    "                                              t_emb_dim=None,\n",
    "                                              num_heads=self.num_heads,\n",
    "                                              num_layers=self.num_mid_layers,\n",
    "                                              norm_channels=self.norm_channels))\n",
    "\n",
    "        self.encoder_norm_out = nn.GroupNorm(self.norm_channels, self.down_channels[-1])\n",
    "        self.encoder_conv_out = nn.Conv2d(self.down_channels[-1], self.z_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.pre_quant_conv = nn.Conv2d(self.z_channels, self.z_channels, kernel_size=1)\n",
    "\n",
    "        self.embedding = nn.Embedding(self.codebook_size, self.z_channels)\n",
    "        ####################################################\n",
    "\n",
    "        ##################### Decoder ######################\n",
    "\n",
    "        self.post_quant_conv = nn.Conv2d(self.z_channels, self.z_channels, kernel_size=1)\n",
    "        self.decoder_conv_in = nn.Conv2d(self.z_channels, self.mid_channels[-1], kernel_size=3, padding=(1, 1))\n",
    "\n",
    "        # Midblock + Upblock\n",
    "        self.decoder_mids = nn.ModuleList([])\n",
    "        for i in reversed(range(1, len(self.mid_channels))):\n",
    "            self.decoder_mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i - 1],\n",
    "                                              t_emb_dim=None,\n",
    "                                              num_heads=self.num_heads,\n",
    "                                              num_layers=self.num_mid_layers,\n",
    "                                              norm_channels=self.norm_channels))\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([])\n",
    "        for i in reversed(range(1, len(self.down_channels))):\n",
    "            self.decoder_layers.append(UpBlock(self.down_channels[i], self.down_channels[i - 1],\n",
    "                                               t_emb_dim=None, up_sample=self.down_sample[i - 1],\n",
    "                                               num_heads=self.num_heads,\n",
    "                                               num_layers=self.num_up_layers,\n",
    "                                               attn=self.attns[i-1],\n",
    "                                               norm_channels=self.norm_channels))\n",
    "\n",
    "        self.decoder_norm_out = nn.GroupNorm(self.norm_channels, self.down_channels[0])\n",
    "        self.decoder_conv_out = nn.Conv2d(self.down_channels[0], self.im_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def quantize(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # B, C, H, W -> B, H, W, C\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "\n",
    "        # B, H, W, C -> B, H*W, C\n",
    "        x = x.reshape(x.size(0), -1, x.size(-1))\n",
    "\n",
    "        # nearest embedding/codebook vector\n",
    "        dist = torch.cdist(x, self.embedding.weight[None, :].repeat((x.size(0), 1, 1)))\n",
    "        # (B, H*W)\n",
    "        min_encoding_indices = torch.argmin(dist, dim=-1)\n",
    "\n",
    "        # replacing encoder output with nearest codebook\n",
    "        quant_out = torch.index_select(self.embedding.weight, 0, min_encoding_indices.view(-1))\n",
    "\n",
    "        # x -> B*H*W, C\n",
    "        x = x.reshape((-1, x.size(-1)))\n",
    "        commmitment_loss = torch.mean((quant_out.detach() - x) ** 2)\n",
    "        codebook_loss = torch.mean((quant_out - x.detach()) ** 2)\n",
    "        quantize_losses = {\n",
    "            'codebook_loss': codebook_loss,\n",
    "            'commitment_loss': commmitment_loss\n",
    "        }\n",
    "        quant_out = x + (quant_out - x).detach()\n",
    "\n",
    "        # quant_out -> B, C, H, W\n",
    "        quant_out = quant_out.reshape((B, H, W, C)).permute(0, 3, 1, 2)\n",
    "        min_encoding_indices = min_encoding_indices.reshape((-1, quant_out.size(-2), quant_out.size(-1)))\n",
    "        return quant_out, quantize_losses, min_encoding_indices\n",
    "\n",
    "    def encode(self, x):\n",
    "        out = self.encoder_conv_in(x)\n",
    "        for idx, down in enumerate(self.encoder_layers):\n",
    "            out = down(out)\n",
    "        for mid in self.encoder_mids:\n",
    "            out = mid(out)\n",
    "        out = self.encoder_norm_out(out)\n",
    "        out = nn.SiLU()(out)\n",
    "        out = self.encoder_conv_out(out)\n",
    "        out = self.pre_quant_conv(out)\n",
    "        out, quant_losses, _ = self.quantize(out)\n",
    "        return out, quant_losses\n",
    "\n",
    "    def decode(self, z):\n",
    "        out = z\n",
    "        out = self.post_quant_conv(out)\n",
    "        out = self.decoder_conv_in(out)\n",
    "        for mid in self.decoder_mids:\n",
    "            out = mid(out)\n",
    "        for idx, up in enumerate(self.decoder_layers):\n",
    "            out = up(out)\n",
    "\n",
    "        out = self.decoder_norm_out(out)\n",
    "        out = nn.SiLU()(out)\n",
    "        out = self.decoder_conv_out(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, quant_losses = self.encode(x)\n",
    "        out = self.decode(z)\n",
    "        return out, z, quant_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2q1aEzLWDUY"
   },
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T12:47:49.636488Z",
     "iopub.status.busy": "2025-09-26T12:47:49.636244Z",
     "iopub.status.idle": "2025-09-26T12:47:49.654942Z",
     "shell.execute_reply": "2025-09-26T12:47:49.654153Z",
     "shell.execute_reply.started": "2025-09-26T12:47:49.636466Z"
    },
    "id": "6VVg0ZkJWDUY",
    "outputId": "aa935b7f-0fcf-4627-d74c-3cdaaef2d30a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchGAN Discriminator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, im_channels=3,\n",
    "                 conv_channels=[64, 128, 256],\n",
    "                 kernels=[4,4,4,4],\n",
    "                 strides=[2,2,2,1],\n",
    "                 paddings=[1,1,1,1]):\n",
    "        super().__init__()\n",
    "        self.im_channels = im_channels\n",
    "        activation = nn.LeakyReLU(0.2)\n",
    "        layers_dim = [self.im_channels] + conv_channels + [1]\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(layers_dim[i], layers_dim[i + 1],\n",
    "                          kernel_size=kernels[i],\n",
    "                          stride=strides[i],\n",
    "                          padding=paddings[i],\n",
    "                          bias=False if i !=0 else True),\n",
    "                nn.BatchNorm2d(layers_dim[i + 1]) if i != len(layers_dim) - 2 and i != 0 else nn.Identity(),\n",
    "                activation if i != len(layers_dim) - 2 else nn.Identity()\n",
    "            )\n",
    "            for i in range(len(layers_dim) - 1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmrbLXn5WDUZ"
   },
   "source": [
    "# lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T12:47:49.656017Z",
     "iopub.status.busy": "2025-09-26T12:47:49.655773Z",
     "iopub.status.idle": "2025-09-26T12:47:49.783888Z",
     "shell.execute_reply": "2025-09-26T12:47:49.783029Z",
     "shell.execute_reply.started": "2025-09-26T12:47:49.655996Z"
    },
    "id": "95ipaqqKWDUZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def spatial_average(in_tens, keepdim=True):\n",
    "    return in_tens.mean([2, 3], keepdim=keepdim)\n",
    "\n",
    "\n",
    "class vgg16(torch.nn.Module):\n",
    "    def __init__(self, requires_grad=False, pretrained=True):\n",
    "        super(vgg16, self).__init__()\n",
    "\n",
    "        vgg_pretrained_features = torchvision.models.vgg16(pretrained=pretrained).features\n",
    "        self.slice1 = torch.nn.Sequential()\n",
    "        self.slice2 = torch.nn.Sequential()\n",
    "        self.slice3 = torch.nn.Sequential()\n",
    "        self.slice4 = torch.nn.Sequential()\n",
    "        self.slice5 = torch.nn.Sequential()\n",
    "        self.N_slices = 5\n",
    "        for x in range(4):\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(4, 9):\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(9, 16):\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(16, 23):\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(23, 30):\n",
    "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
    "\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = self.slice1(X)\n",
    "        h_relu1_2 = h\n",
    "        h = self.slice2(h)\n",
    "        h_relu2_2 = h\n",
    "        h = self.slice3(h)\n",
    "        h_relu3_3 = h\n",
    "        h = self.slice4(h)\n",
    "        h_relu4_3 = h\n",
    "        h = self.slice5(h)\n",
    "        h_relu5_3 = h\n",
    "        vgg_outputs = namedtuple(\"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])\n",
    "        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LPIPS(nn.Module):\n",
    "    def __init__(self, net='vgg', version='0.1', use_dropout=True):\n",
    "        super(LPIPS, self).__init__()\n",
    "        self.version = version\n",
    "        self.scaling_layer = ScalingLayer()\n",
    "\n",
    "        self.chns = [64, 128, 256, 512, 512]\n",
    "        self.L = len(self.chns)\n",
    "        self.net = vgg16(pretrained=True, requires_grad=False)\n",
    "\n",
    "        self.lin0 = NetLinLayer(self.chns[0], use_dropout=use_dropout)\n",
    "        self.lin1 = NetLinLayer(self.chns[1], use_dropout=use_dropout)\n",
    "        self.lin2 = NetLinLayer(self.chns[2], use_dropout=use_dropout)\n",
    "        self.lin3 = NetLinLayer(self.chns[3], use_dropout=use_dropout)\n",
    "        self.lin4 = NetLinLayer(self.chns[4], use_dropout=use_dropout)\n",
    "        self.lins = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]\n",
    "        self.lins = nn.ModuleList(self.lins)\n",
    "\n",
    "        # Load weights of trained LPIPS model\n",
    "        model_path = '...'\n",
    "        print('Loading model from: %s' % model_path)\n",
    "        self.load_state_dict(torch.load(model_path, map_location=device, weights_only = True), strict=False)\n",
    "\n",
    "        self.eval()\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, in0, in1, normalize=False):\n",
    "        if normalize:  \n",
    "            in0 = 2 * in0 - 1\n",
    "            in1 = 2 * in1 - 1\n",
    "\n",
    "        in0_input, in1_input = self.scaling_layer(in0), self.scaling_layer(in1)\n",
    "\n",
    "        outs0, outs1 = self.net.forward(in0_input), self.net.forward(in1_input)\n",
    "        feats0, feats1, diffs = {}, {}, {}\n",
    "\n",
    "        for kk in range(self.L):\n",
    "            feats0[kk], feats1[kk] = torch.nn.functional.normalize(outs0[kk], dim=1), torch.nn.functional.normalize(\n",
    "                outs1[kk])\n",
    "            diffs[kk] = (feats0[kk] - feats1[kk]) ** 2\n",
    "\n",
    "        res = [spatial_average(self.lins[kk](diffs[kk]), keepdim=True) for kk in range(self.L)]\n",
    "        val = 0\n",
    "\n",
    "        for l in range(self.L):\n",
    "            val += res[l]\n",
    "        return val\n",
    "\n",
    "\n",
    "class ScalingLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScalingLayer, self).__init__()\n",
    "        # Imagnet normalization for (0-1)\n",
    "        # mean = [0.485, 0.456, 0.406]\n",
    "        # std = [0.229, 0.224, 0.225]\n",
    "        self.register_buffer('shift', torch.Tensor([-.030, -.088, -.188])[None, :, None, None])\n",
    "        self.register_buffer('scale', torch.Tensor([.458, .448, .450])[None, :, None, None])\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return (inp - self.shift) / self.scale\n",
    "\n",
    "\n",
    "class NetLinLayer(nn.Module):\n",
    "    ''' A single linear layer which does a 1x1 conv '''\n",
    "\n",
    "    def __init__(self, chn_in, chn_out=1, use_dropout=False):\n",
    "        super(NetLinLayer, self).__init__()\n",
    "\n",
    "        layers = [nn.Dropout(), ] if (use_dropout) else []\n",
    "        layers += [nn.Conv2d(chn_in, chn_out, 1, stride=1, padding=0, bias=False), ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAi94d8mWDUa"
   },
   "source": [
    "# Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T12:47:49.785920Z",
     "iopub.status.busy": "2025-09-26T12:47:49.785690Z",
     "iopub.status.idle": "2025-09-26T12:47:49.811389Z",
     "shell.execute_reply": "2025-09-26T12:47:49.810649Z",
     "shell.execute_reply.started": "2025-09-26T12:47:49.785903Z"
    },
    "id": "B77C7AXOWDUb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TacoDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, resolution, random_crop=False, random_flip=True, is_train=True, split=\"train\"):\n",
    "        \"\"\"\n",
    "        Dataset per TACO, con suddivisione in train/val/test e shuffle iniziale.\n",
    "\n",
    "        :param image_dir: Directory delle immagini.\n",
    "        :param mask_dir: Directory delle maschere.\n",
    "        :param resolution: Dimensione target (H, W) per il ridimensionamento.\n",
    "        :param random_crop: Se True, applica random crop.\n",
    "        :param random_flip: Se True, applica random flip.\n",
    "        :param is_train: Se True, abilita augmentation.\n",
    "        :param split: Specifica se train, val o test.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.resolution = resolution\n",
    "        self.random_crop = random_crop\n",
    "        self.random_flip = random_flip\n",
    "        self.is_train = is_train\n",
    "\n",
    "        all_images = sorted(os.listdir(image_dir))\n",
    "        all_masks = sorted(os.listdir(mask_dir))\n",
    "\n",
    "        assert len(all_images) == len(all_masks), \"Mismatch tra immagini e maschere!\"\n",
    "\n",
    "        # Shuffle iniziale per evitare bias nell'ordine dei file\n",
    "        combined = list(zip(all_images, all_masks))\n",
    "        random.shuffle(combined)\n",
    "        all_images, all_masks = zip(*combined)\n",
    "\n",
    "        # Suddivisione in train (1000), val (250), test (250)\n",
    "        train_split = int(3181)\n",
    "\n",
    "        if split == \"train\":\n",
    "            self.image_paths = all_images[:train_split]\n",
    "            self.mask_paths = all_masks[:train_split]\n",
    "        else:\n",
    "            raise ValueError(\"split deve essere 'train'.\")\n",
    "\n",
    "        print(f\"Dataset {split} istanziato con {len(self)} campioni.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_paths[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_paths[idx])\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        if self.is_train:\n",
    "            if self.random_crop:\n",
    "                image, mask = self.random_crop_arr(image, mask)\n",
    "            else:\n",
    "                image, mask = self.center_crop_arr(image, mask)\n",
    "        else:\n",
    "            image, mask = self.resize_arr(image, mask, keep_aspect=False)\n",
    "\n",
    "        if self.random_flip and random.random() < 0.5:\n",
    "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        image = np.array(image).astype(np.float32) / 127.5 - 1\n",
    "        mask = np.array(mask).astype(np.int64)\n",
    "\n",
    "        return np.transpose(image, (2, 0, 1)), {\"label\": torch.from_numpy(mask).unsqueeze(0)}\n",
    "    def resize_arr(self, image, mask, keep_aspect=True):\n",
    "        \"\"\"\n",
    "        Ridimensiona immagine e maschera alla risoluzione target.\n",
    "        \"\"\"\n",
    "        if keep_aspect:\n",
    "            scale = self.resolution / min(image.size)\n",
    "            image = image.resize((round(image.size[0] * scale), round(image.size[1] * scale)), Image.BICUBIC)\n",
    "        else:\n",
    "            image = image.resize((self.resolution, self.resolution), Image.BICUBIC)\n",
    "\n",
    "        mask = mask.resize(image.size, Image.NEAREST)\n",
    "        return image, mask\n",
    "\n",
    "    def center_crop_arr(self, image, mask):\n",
    "        \"\"\"\n",
    "        Esegue un ritaglio centrato.\n",
    "        \"\"\"\n",
    "        scale = self.resolution / min(image.size)\n",
    "        image = image.resize((round(image.size[0] * scale), round(image.size[1] * scale)), Image.BICUBIC)\n",
    "        mask = mask.resize(image.size, Image.NEAREST)\n",
    "\n",
    "        crop_x = (image.size[0] - self.resolution) // 2\n",
    "        crop_y = (image.size[1] - self.resolution) // 2\n",
    "        image = image.crop((crop_x, crop_y, crop_x + self.resolution, crop_y + self.resolution))\n",
    "        mask = mask.crop((crop_x, crop_y, crop_x + self.resolution, crop_y + self.resolution))\n",
    "        return image, mask\n",
    "\n",
    "    def random_crop_arr(self, image, mask):\n",
    "        \"\"\"\n",
    "        Esegue un ritaglio casuale.\n",
    "        \"\"\"\n",
    "        scale = self.resolution / min(image.size)\n",
    "        image = image.resize((round(image.size[0] * scale), round(image.size[1] * scale)), Image.BICUBIC)\n",
    "        mask = mask.resize(image.size, Image.NEAREST)\n",
    "\n",
    "        crop_x = random.randint(0, image.size[0] - self.resolution)\n",
    "        crop_y = random.randint(0, image.size[1] - self.resolution)\n",
    "        image = image.crop((crop_x, crop_y, crop_x + self.resolution, crop_y + self.resolution))\n",
    "        mask = mask.crop((crop_x, crop_y, crop_x + self.resolution, crop_y + self.resolution))\n",
    "        return image, mask\n",
    "\n",
    "def load_taco_data(image_path, mask_path, batch_size, image_size, split):\n",
    "    \"\"\"\n",
    "    Genera un DataLoader per il dataset TACO.\n",
    "\n",
    "    :param split: 'train', 'val' o 'test'\n",
    "    \"\"\"\n",
    "    dataset = TacoDataset(\n",
    "        image_dir=image_path,\n",
    "        mask_dir=mask_path,\n",
    "        resolution=image_size,\n",
    "        random_crop=(split == \"train\"),\n",
    "        random_flip=(split == \"train\"),\n",
    "        is_train=(split == \"train\"),\n",
    "        split=split\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=(split == \"train\"), num_workers=2, drop_last=True)\n",
    "    return loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T12:47:49.812373Z",
     "iopub.status.busy": "2025-09-26T12:47:49.812072Z",
     "iopub.status.idle": "2025-09-26T12:47:49.985850Z",
     "shell.execute_reply": "2025-09-26T12:47:49.985067Z",
     "shell.execute_reply.started": "2025-09-26T12:47:49.812350Z"
    },
    "id": "xK1KtMgWWDUb",
    "outputId": "c47dd6d8-6eaa-4db1-dd4f-0480771210af",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_path = '...'\n",
    "masks_path = '...'\n",
    "train_loader = load_taco_data(image_path, masks_path, batch_size=8, image_size=256, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T12:47:49.987099Z",
     "iopub.status.busy": "2025-09-26T12:47:49.986808Z",
     "iopub.status.idle": "2025-09-26T12:47:53.698366Z",
     "shell.execute_reply": "2025-09-26T12:47:53.697390Z",
     "shell.execute_reply.started": "2025-09-26T12:47:49.987071Z"
    },
    "id": "8yIwHuplaJct",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "images, cond = next(iter(train_loader))\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T12:47:53.699572Z",
     "iopub.status.busy": "2025-09-26T12:47:53.699350Z",
     "iopub.status.idle": "2025-09-26T12:47:53.989049Z",
     "shell.execute_reply": "2025-09-26T12:47:53.988373Z",
     "shell.execute_reply.started": "2025-09-26T12:47:53.699551Z"
    },
    "id": "b0LqY6mMaLnE",
    "outputId": "da8c5eac-f6b1-4ad2-94a9-b057b11ce867",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "img = (images[i].permute(1, 2, 0).numpy() + 1) / 2.0  \n",
    "mask = cond['label'][i, 0].numpy()  \n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Image\")\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Mask\")\n",
    "plt.imshow(mask)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(images.shape)\n",
    "print(cond['label'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEbkRFX2WDUc"
   },
   "source": [
    "# train VQVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T12:47:53.990104Z",
     "iopub.status.busy": "2025-09-26T12:47:53.989894Z",
     "iopub.status.idle": "2025-09-26T12:47:54.006622Z",
     "shell.execute_reply": "2025-09-26T12:47:54.005881Z",
     "shell.execute_reply.started": "2025-09-26T12:47:53.990088Z"
    },
    "id": "6lRtTCG_WDUc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train(data, start_epoch ,num_epochs, save_path,\n",
    "          vqvae_ckpt_path=None,\n",
    "          discriminator_ckpt_path=None,\n",
    "          optimizer_g_ckpt_path=None,\n",
    "          optimizer_d_ckpt_path=None,\n",
    "          step_count_ckpt_path=None):\n",
    "\n",
    "    vqvae_model = VQVAE().to(device)\n",
    "    lpips_model = LPIPS().eval().to(device)\n",
    "    discriminator = Discriminator().to(device)\n",
    "\n",
    "    reconstruction_criterion = torch.nn.MSELoss()\n",
    "    discriminator_criterion = torch.nn.MSELoss()\n",
    "\n",
    "    optimizer_d = Adam(discriminator.parameters(), lr=3e-5, betas=(0.5, 0.999))\n",
    "    optimizer_g = Adam(vqvae_model.parameters(), lr=3e-5, betas=(0.5, 0.999))\n",
    "    discriminator_step_start = 15000\n",
    "    step_count = 0\n",
    "\n",
    "    # Ripristino checkpoint\n",
    "    if vqvae_ckpt_path and os.path.exists(vqvae_ckpt_path):\n",
    "        print(f\"Loading VQVAE weights from: {vqvae_ckpt_path}\")\n",
    "        vqvae_model.load_state_dict(torch.load(vqvae_ckpt_path, map_location=device, weights_only = True))\n",
    "\n",
    "    if discriminator_ckpt_path and os.path.exists(discriminator_ckpt_path):\n",
    "        print(f\"Loading Discriminator weights from: {discriminator_ckpt_path}\")\n",
    "        discriminator.load_state_dict(torch.load(discriminator_ckpt_path, map_location=device, weights_only = True))\n",
    "\n",
    "    if optimizer_g_ckpt_path and os.path.exists(optimizer_g_ckpt_path):\n",
    "        print(f\"Loading Generator optimizer state from: {optimizer_g_ckpt_path}\")\n",
    "        optimizer_g.load_state_dict(torch.load(optimizer_g_ckpt_path, map_location=device, weights_only = True))\n",
    "\n",
    "    if optimizer_d_ckpt_path and os.path.exists(optimizer_d_ckpt_path):\n",
    "        print(f\"Loading Discriminator optimizer state from: {optimizer_d_ckpt_path}\")\n",
    "        optimizer_d.load_state_dict(torch.load(optimizer_d_ckpt_path, map_location=device, weights_only = True))\n",
    "\n",
    "    if step_count_ckpt_path and os.path.exists(step_count_ckpt_path):\n",
    "        print(f\"Loading step count from: {step_count_ckpt_path}\")\n",
    "        step_count = torch.load(step_count_ckpt_path, weights_only = True)\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "        pbar = tqdm(data, desc=f\"Epoch {epoch+1}/{start_epoch + num_epochs}\")\n",
    "        epoch_generator_losses = []\n",
    "        epoch_discriminator_losses = []\n",
    "\n",
    "        for image, _ in pbar:\n",
    "            optimizer_g.zero_grad()\n",
    "            optimizer_d.zero_grad()\n",
    "\n",
    "            step_count += 1\n",
    "            image = image.float().to(device)\n",
    "\n",
    "            # Generator\n",
    "            model_output = vqvae_model(image)\n",
    "            output, z, quantize_losses = model_output\n",
    "\n",
    "            reconstruction_loss = reconstruction_criterion(output, image)\n",
    "            generator_loss = (reconstruction_loss +\n",
    "                              (1 * quantize_losses['codebook_loss']) +\n",
    "                              (0.2 * quantize_losses['commitment_loss']) )\n",
    "\n",
    "            if step_count > discriminator_step_start:\n",
    "                discriminator_fake_pred = discriminator(output)\n",
    "                discriminator_fake_loss = discriminator_criterion(\n",
    "                    discriminator_fake_pred,\n",
    "                    torch.ones_like(discriminator_fake_pred)\n",
    "                )\n",
    "                generator_loss += 0.5 * discriminator_fake_loss\n",
    "\n",
    "            lpips_loss = torch.mean(lpips_model(output, image))\n",
    "            generator_loss += lpips_loss\n",
    "            generator_loss.backward()\n",
    "            optimizer_g.step()\n",
    "\n",
    "            generator_loss_value = generator_loss.item()\n",
    "            epoch_generator_losses.append(generator_loss_value)\n",
    "            discriminator_loss_value = 0.0\n",
    "\n",
    "            # Discriminator\n",
    "            if step_count > discriminator_step_start:\n",
    "                fake = output.detach()\n",
    "                discriminator_fake_pred = discriminator(fake)\n",
    "                discriminator_real_pred = discriminator(image)\n",
    "\n",
    "                discriminator_fake_loss = discriminator_criterion(\n",
    "                    discriminator_fake_pred,\n",
    "                    torch.zeros_like(discriminator_fake_pred)\n",
    "                )\n",
    "                discriminator_real_loss = discriminator_criterion(\n",
    "                    discriminator_real_pred,\n",
    "                    torch.ones_like(discriminator_real_pred)\n",
    "                )\n",
    "                discriminator_loss = 0.5 * (discriminator_fake_loss + discriminator_real_loss) / 2\n",
    "                discriminator_loss.backward()\n",
    "                optimizer_d.step()\n",
    "                discriminator_loss_value = discriminator_loss.item()\n",
    "                epoch_discriminator_losses.append(discriminator_loss_value)\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"G_loss\": f\"{generator_loss_value:.4f}\",\n",
    "                \"D_loss\": f\"{discriminator_loss_value:.4f}\"\n",
    "            })\n",
    "\n",
    "        avg_g_loss = sum(epoch_generator_losses) / len(epoch_generator_losses)\n",
    "        if epoch_discriminator_losses:\n",
    "            avg_d_loss = sum(epoch_discriminator_losses) / len(epoch_discriminator_losses)\n",
    "        else:\n",
    "            avg_d_loss = 0.0\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Average G_loss = {avg_g_loss:.4f}, Average D_loss = {avg_d_loss:.4f}\")\n",
    "\n",
    "        # Salvataggio checkpoint\n",
    "        torch.save(vqvae_model.state_dict(), os.path.join(save_path, 'vqvae_1_ckpt.pth'))\n",
    "        torch.save(discriminator.state_dict(), os.path.join(save_path, 'discriminator_1_ckpt.pth'))\n",
    "        torch.save(optimizer_g.state_dict(), os.path.join(save_path, 'optimizer_g_1_ckpt.pth'))\n",
    "        torch.save(optimizer_d.state_dict(), os.path.join(save_path, 'optimizer_d_1_ckpt.pth'))\n",
    "        torch.save(step_count, os.path.join(save_path, 'step_count.pth'))\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            vqvae_model.eval()\n",
    "            with torch.no_grad():\n",
    "                sample_images, _ = next(iter(data))\n",
    "                sample_images = sample_images.float().to(device)\n",
    "                reconstructed, _, _ = vqvae_model(sample_images)\n",
    "\n",
    "                save_image(sample_images, os.path.join(save_path + '/images', f\"epoch_{epoch+1}_real.png\"), nrow=4, normalize=True)\n",
    "                save_image(reconstructed, os.path.join(save_path + '/images', f\"epoch_{epoch+1}_reconstructed.png\"), nrow=4, normalize=True)\n",
    "\n",
    "                vqvae_model.train()\n",
    "\n",
    "    print(\"Training done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjNOma1UWDUc"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T12:48:29.047373Z",
     "iopub.status.busy": "2025-09-26T12:48:29.046844Z"
    },
    "id": "Fg8eHRFaWDUd",
    "outputId": "e56b03a0-75cb-4c63-bb26-694a3a588966",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train(train_loader, start_epoch=0 ,num_epochs=1000, save_path='...',\n",
    "          vqvae_ckpt_path='...',\n",
    "          discriminator_ckpt_path='...',\n",
    "          optimizer_g_ckpt_path='...',\n",
    "          optimizer_d_ckpt_path='...',\n",
    "          step_count_ckpt_path='...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T15:54:24.528049Z",
     "iopub.status.busy": "2025-09-25T15:54:24.527753Z",
     "iopub.status.idle": "2025-09-25T15:54:29.619612Z",
     "shell.execute_reply": "2025-09-25T15:54:29.618732Z",
     "shell.execute_reply.started": "2025-09-25T15:54:24.528024Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def show_images(original, reconstructed, n=8):\n",
    "    original = original[:n]\n",
    "    reconstructed = reconstructed[:n]\n",
    "\n",
    "    original = make_grid(original, nrow=n, normalize=True).permute(1, 2, 0).cpu().numpy()\n",
    "    reconstructed = make_grid(reconstructed, nrow=n, normalize=True).permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(2*n, 4))\n",
    "    axes[0].imshow(original)\n",
    "    axes[0].set_title(\"Original\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(reconstructed)\n",
    "    axes[1].set_title(\"Reconstructed\")\n",
    "    axes[1].axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def test_vqvae(checkpoint_path):\n",
    "    model = VQVAE().to(device)\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        images, _ = next(iter(train_loader))\n",
    "        images = images.to(device)\n",
    "\n",
    "        output, _, _ = model(images)\n",
    "        show_images(images, output)\n",
    "    return images, output\n",
    "\n",
    "images, output = test_vqvae('...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esempio singolo Immagine originale - Ricostruzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T15:54:36.879501Z",
     "iopub.status.busy": "2025-09-25T15:54:36.879206Z",
     "iopub.status.idle": "2025-09-25T15:54:37.164900Z",
     "shell.execute_reply": "2025-09-25T15:54:37.164196Z",
     "shell.execute_reply.started": "2025-09-25T15:54:36.879476Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "im = output[6].cpu()\n",
    "img1 = images[6].cpu()\n",
    "\n",
    "min_val = im.min()\n",
    "max_val = im.max()\n",
    "\n",
    "img_norm = (im - min_val) / (max_val - min_val)\n",
    "img_norm = np.clip(img_norm, 0.0, 1.0)\n",
    "\n",
    "min_val = img1.min()\n",
    "max_val = img1.max()\n",
    "\n",
    "img1_norm = (img1 - min_val) / (max_val - min_val)\n",
    "img1_norm = np.clip(img1_norm, 0.0, 1.0)\n",
    "\n",
    "def plot_images(img1, img2, titles=['Image 1', 'Image 2']):\n",
    "    plt.figure(figsize=(7, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img1)\n",
    "    plt.title(titles[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img2)\n",
    "    plt.title(titles[1])\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_images(img1_norm.permute(1,2,0), img_norm.permute(1,2,0), titles=['Original', 'vae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T09:15:43.950451Z",
     "iopub.status.busy": "2025-09-20T09:15:43.949760Z",
     "iopub.status.idle": "2025-09-20T09:17:15.972970Z",
     "shell.execute_reply": "2025-09-20T09:17:15.971972Z",
     "shell.execute_reply.started": "2025-09-20T09:15:43.950423Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install torch torchvision torchmetrics scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T10:04:04.038298Z",
     "iopub.status.busy": "2025-09-20T10:04:04.037744Z",
     "iopub.status.idle": "2025-09-20T10:05:28.459354Z",
     "shell.execute_reply": "2025-09-20T10:05:28.458269Z",
     "shell.execute_reply.started": "2025-09-20T10:04:04.038273Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install torchmetrics[image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T10:17:15.995247Z",
     "iopub.status.busy": "2025-09-20T10:17:15.994341Z",
     "iopub.status.idle": "2025-09-20T10:24:22.279287Z",
     "shell.execute_reply": "2025-09-20T10:24:22.278410Z",
     "shell.execute_reply.started": "2025-09-20T10:17:15.995190Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ========== CONFIGURAZIONE ==========\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vqvae = VQVAE().to(device)  \n",
    "vqvae.load_state_dict(torch.load('/kaggle/working/vqvae_1_ckpt.pth', map_location=device))\n",
    "vqvae.to(device).eval()\n",
    "\n",
    "# ========== METRICHE ==========\n",
    "fid = FrechetInceptionDistance(feature=2048).to(device)\n",
    "mse_vals, psnr_vals, ssim_vals = [], [], []\n",
    "\n",
    "# ========== LOOP DI VALUTAZIONE ==========\n",
    "with torch.no_grad():\n",
    "    for x, _ in train_loader:\n",
    "        x = x.to(device)\n",
    "        # --- Forward del VQ-VAE ---\n",
    "        recon, z, quant_losses = vqvae(x)\n",
    "\n",
    "        # --- Normalizzazione [-1,1] -> [0,1] ---\n",
    "        x_norm = (x + 1) / 2\n",
    "        recon_norm = (recon + 1) / 2\n",
    "        x_norm = torch.clamp(x_norm, 0, 1)\n",
    "        recon_norm = torch.clamp(recon_norm, 0, 1)\n",
    "\n",
    "        # --- Aggiorna FID convertendo in uint8 ---\n",
    "        x_uint8 = (x_norm * 255).to(torch.uint8)\n",
    "        recon_uint8 = (recon_norm * 255).to(torch.uint8)\n",
    "        fid.update(x_uint8, real=True)\n",
    "        fid.update(recon_uint8, real=False)\n",
    "\n",
    "        # --- Converti a numpy per PSNR/SSIM ---\n",
    "        x_np = x_norm.detach().cpu().numpy()\n",
    "        recon_np = recon_norm.detach().cpu().numpy()\n",
    "\n",
    "        for i in range(x_np.shape[0]):\n",
    "            orig = np.transpose(x_np[i], (1,2,0))\n",
    "            rec = np.transpose(recon_np[i], (1,2,0))\n",
    "            # MSE\n",
    "            mse = np.mean((orig - rec) ** 2)\n",
    "            mse_vals.append(mse)\n",
    "            # PSNR\n",
    "            if mse != 0:\n",
    "                psnr_vals.append(10 * log10(1.0 / mse))\n",
    "            else:\n",
    "                psnr_vals.append(float('inf'))\n",
    "            # SSIM con win_size corretto\n",
    "            H, W = orig.shape[:2]\n",
    "            win_size = 7\n",
    "            if min(H, W) < win_size:\n",
    "                win_size = min(H, W) if min(H, W) % 2 == 1 else min(H, W) - 1\n",
    "            ssim_score = ssim(orig, rec, data_range=1.0, channel_axis=2, win_size=win_size)\n",
    "            ssim_vals.append(ssim_score)\n",
    "\n",
    "# ========== RISULTATI ==========\n",
    "fid_score = fid.compute().item()\n",
    "psnr_mean = np.mean(psnr_vals)\n",
    "ssim_mean = np.mean(ssim_vals)\n",
    "\n",
    "print(f\"FID  : {fid_score:.4f}\")\n",
    "print(f\"PSNR : {psnr_mean:.4f} dB\")\n",
    "print(f\"SSIM : {ssim_mean:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "MPn133poWDUR",
    "7CyFTKi_WDUW",
    "I2q1aEzLWDUY",
    "EmrbLXn5WDUZ",
    "lAi94d8mWDUa",
    "MEbkRFX2WDUc"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7316062,
     "sourceId": 11658220,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8248919,
     "sourceId": 13027815,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8339295,
     "sourceId": 13160979,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
