{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["22VznaGUNRS9","r76crKkwfYWv","S5tNQJJxGwPd","pLI3XqCU7Skm","Lgid7NhLGz30","XX5-puDnYuyw","JGTKkOX6u65y"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"EpfSHDWqqht2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757923385428,"user_tz":-120,"elapsed":20190,"user":{"displayName":"davide","userId":"08345280063963440379"}},"outputId":"909e639b-8582-43e1-b18c-8d9e617b7563"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"markdown","source":["# Conteggio macro categorie del dataset originale"],"metadata":{"id":"22VznaGUNRS9"}},{"cell_type":"code","source":["import json\n","import os\n","from collections import defaultdict\n","\n","# Percorso della cartella delle annotazioni\n","annotations_dir = '/content/drive/MyDrive/Colab Notebooks/TACO dataset/ds/ann'\n","\n","# Lista delle classi\n","classes = [\n","    'background', \"Plastic bag & wrapper\", \"Cup\", \"Plastic gloves\", \"Styrofoam piece\",\n","    \"Aluminium foil\", \"Shoe\", \"Unlabeled litter\", \"Lid\", \"Rope & strings\",\n","    \"Broken glass\", \"Paper bag\", \"Blister pack\", \"Carton\", \"Bottle cap\", \"Paper\",\n","    \"Plastic container\", \"Pop tab\", \"Straw\", \"Bottle\", \"Plastic utensils\",\n","    \"Other plastic\", \"Glass jar\", \"Battery\", \"Food waste\", \"Scrap metal\",\n","    \"Can\", \"Cigarette\", \"Squeezable tube\"\n","]\n","\n","# Dizionario per tenere traccia del conteggio delle classi\n","class_counts = defaultdict(int)\n","\n","# Set per tracciare gli oggetti già conteggiati per ID\n","counted_object_ids = set()\n","\n","# Itera su ogni file nella directory delle annotazioni\n","for filename in os.listdir(annotations_dir):\n","    if filename.endswith('.json'):  # Considera solo i file JSON\n","        file_path = os.path.join(annotations_dir, filename)\n","        with open(file_path, 'r') as f:\n","            data = json.load(f)\n","\n","            # Itera sugli oggetti presenti nelle annotazioni\n","            for obj in data.get(\"objects\", []):\n","                # Trova il valore del supercategory dai tag dell'oggetto\n","                supercategory = None\n","                for tag in obj.get(\"tags\", []):\n","                    if tag.get(\"name\") == \"supercategory\":\n","                        supercategory = tag.get(\"value\")\n","                        break\n","\n","                # Controlla se l'oggetto ha più di 2 punti (per i contorni)\n","                points = obj.get(\"points\", {}).get(\"exterior\", [])\n","                if len(points) > 2:\n","                    # Se l'oggetto non è già stato contato\n","                    if supercategory in classes and obj['id'] not in counted_object_ids:\n","                        # Incrementa il conteggio per la classe\n","                        class_counts[supercategory] += 1\n","                        # Aggiungi l'ID dell'oggetto al set per evitare il doppio conteggio\n","                        counted_object_ids.add(obj['id'])\n","\n","# Stampa il conteggio totale per ogni classe\n","print(\"Conteggio oggetti per classe:\")\n","for cls in classes:\n","    print(f\"{cls}: {class_counts[cls]}\")\n"],"metadata":{"id":"twqP_Io_NCFU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Download immagini e maschere"],"metadata":{"id":"r76crKkwfYWv"}},{"cell_type":"code","source":["def download_taco_images_and_masks(annotations_path, target_class=None, max_images=None, image_output_dir=None, mask_output_dir=None, skip_existing=True):\n","    \"\"\"\n","    Scarica immagini da TACO e genera maschere semantiche basate sulle categorie secondo class_finals.\n","    Gestisce correttamente l'orientamento EXIF per evitare rotazioni tra immagini e maschere.\n","\n","    NOVITÀ: Può filtrare per una classe specifica e limitare il numero di immagini scaricate.\n","\n","    Args:\n","        annotations_path (str): Path alle annotazioni json\n","        target_class (int, optional): ID della classe da scaricare (es. 4 per carta/cartone). Se None, scarica tutte.\n","        max_images (int, optional): Numero massimo di immagini da scaricare per la classe target. Se None, nessun limite.\n","        image_output_dir (str, optional): Cartella per immagini. Default = stessa di annotations.\n","        mask_output_dir (str, optional): Cartella per maschere. Default = image_output_dir/masks\n","        skip_existing (bool): Se True, salta immagini e maschere già esistenti. Se False, rigenera tutto.\n","\n","    Returns:\n","        int: Numero immagini scaricate\n","    \"\"\"\n","\n","    import os\n","    import json\n","    import requests\n","    import sys\n","    import numpy as np\n","    from PIL import Image, ImageDraw, ImageOps\n","    from io import BytesIO\n","    from collections import defaultdict\n","\n","    # Dizionario fornito dall'utente\n","    class_finals = {\n","        # PLASTICA E POLIMERI (1)\n","    \"Other plastic bottle\": 1,\n","    \"Clear plastic bottle\": 1,\n","    \"Plastic bottle cap\": 1,\n","    \"Spread tub\": 1,\n","    \"Tupperware\": 1,\n","    \"Disposable food container\": 1,\n","    \"Other plastic container\": 1,\n","    \"Plastic film\": 1,\n","    \"Garbage bag\": 1,\n","    \"Single-use carrier bag\": 1,\n","    \"Polypropylene bag\": 1,\n","    \"Plastified paper bag\": 1,\n","    \"Carded blister pack\": 1,\n","    \"Other plastic wrapper\": 1,\n","    \"Crisp packet\": 1,\n","    \"Disposable plastic cup\": 1,\n","    \"Other plastic cup\": 1,\n","    \"Plastic lid\": 1,\n","    \"Plastic glooves\": 1,\n","    \"Plastic utensils\": 1,\n","    \"Plastic straw\": 1,\n","    \"Other plastic\": 1,\n","    \"Six pack rings\": 1,\n","\n","    # METALLI (2)\n","    \"Food Can\": 2,\n","    \"Aerosol\": 2,\n","    \"Drink can\": 2,\n","    \"Metal bottle cap\": 2,\n","    \"Metal lid\": 2,\n","    \"Pop tab\": 2,\n","    \"Scrap metal\": 2,\n","    \"Aluminium foil\": 2,\n","    \"Aluminium blister pack\": 2,\n","\n","    # VETRO (3)\n","    \"Glass bottle\": 3,\n","    \"Glass jar\": 3,\n","    \"Glass cup\": 3,\n","    \"Broken glass\": 3,\n","\n","    # CARTA E CARTONE (4)\n","    \"Other carton\": 4,\n","    \"Egg carton\": 4,\n","    \"Drink carton\": 4,\n","    \"Corrugated carton\": 4,\n","    \"Meal carton\": 4,\n","    \"Pizza box\": 4,\n","    \"Magazine paper\": 4,\n","    \"Normal paper\": 4,\n","    \"Wrapping paper\": 4,\n","    \"Paper bag\": 4,\n","    \"Paper cup\": 4,\n","    \"Paper straw\": 4,\n","    \"Tissues\": 4,\n","    \"Toilet tube\": 4,\n","\n","    # polistirolo (5)\n","    \"Foam cup\": 5,\n","    \"Foam food container\": 5,\n","    \"Styrofoam piece\": 5,\n","\n","    # SIGARETTE (6)\n","    \"Food waste\": 6,\n","    \"Battery\": 6,\n","    \"Cigarette\": 6,\n","\n","    # DA RIMUOVERE (8)\n","    \"Shoe\": 8,\n","    \"Rope & strings\": 8,\n","    \"Squeezable tube\": 8,\n","\n","    # NON CLASSIFICATI (7)\n","    \"Unlabeled litter\": 7\n","    }\n","\n","    def get_exif_rotation_angle(img):\n","        \"\"\"Restituisce l'angolo di rotazione basato sui metadati EXIF\"\"\"\n","        try:\n","            exif = img._getexif()\n","            if exif is not None:\n","                orientation = exif.get(274)  # Tag EXIF per orientamento\n","                if orientation == 3:\n","                    return 180\n","                elif orientation == 6:\n","                    return 270\n","                elif orientation == 8:\n","                    return 90\n","        except:\n","            pass\n","        return 0\n","\n","    def apply_rotation_to_polygon(polygon, angle, width, height):\n","        \"\"\"Applica la rotazione ai punti del poligono\"\"\"\n","        if angle == 0:\n","            return polygon\n","\n","        points = []\n","        for i in range(0, len(polygon), 2):\n","            x, y = polygon[i], polygon[i+1]\n","\n","            if angle == 90:\n","                new_x, new_y = height - y, x\n","                new_width, new_height = height, width\n","            elif angle == 180:\n","                new_x, new_y = width - x, height - y\n","                new_width, new_height = width, height\n","            elif angle == 270:\n","                new_x, new_y = y, width - x\n","                new_width, new_height = height, width\n","            else:\n","                new_x, new_y = x, y\n","                new_width, new_height = width, height\n","\n","            points.extend([new_x, new_y])\n","\n","        return points, new_width, new_height\n","\n","    def image_contains_target_class(image_id, annotations, category_id_to_name, target_class):\n","        \"\"\"Verifica se un'immagine contiene la classe target\"\"\"\n","        anns = image_id_to_anns[image_id]\n","        for ann in anns:\n","            category_id = ann['category_id']\n","            category_name = category_id_to_name.get(category_id)\n","            label_id = class_finals.get(category_name, 0)\n","            if label_id == target_class:\n","                return True\n","        return False\n","\n","    if image_output_dir is None:\n","        image_output_dir = os.path.dirname(annotations_path)\n","    if mask_output_dir is None:\n","        mask_output_dir = os.path.join(image_output_dir, \"masks\")\n","\n","    os.makedirs(image_output_dir, exist_ok=True)\n","    os.makedirs(mask_output_dir, exist_ok=True)\n","\n","    with open(annotations_path, 'r') as f:\n","        annotations = json.load(f)\n","\n","    category_id_to_name = {c['id']: c['name'] for c in annotations['categories']}\n","    image_id_to_anns = defaultdict(list)\n","    for ann in annotations['annotations']:\n","        image_id_to_anns[ann['image_id']].append(ann)\n","\n","    images_to_process = annotations['images']\n","    if target_class is not None:\n","        images_to_process = [img for img in annotations['images']\n","                           if image_contains_target_class(img['id'], annotations, category_id_to_name, target_class)]\n","        print(f\"Trovate {len(images_to_process)} immagini contenenti la classe {target_class}\")\n","\n","    if max_images is not None and len(images_to_process) > max_images:\n","        images_to_process = images_to_process[:max_images]\n","        print(f\"Limitate a {max_images} immagini\")\n","\n","    nr_images = len(images_to_process)\n","    downloaded_count = 0\n","    skipped_count = 0\n","\n","    print(f\"Processando {nr_images} immagini...\")\n","\n","    for i, image in enumerate(images_to_process):\n","        file_name = image['file_name']\n","        url_original = image['flickr_url']\n","        width, height = image['width'], image['height']\n","\n","        subfolder = os.path.dirname(file_name)\n","        base_name = os.path.basename(file_name)\n","        new_file_name = f\"{subfolder}_{base_name}\" if subfolder else base_name\n","\n","        img_path = os.path.join(image_output_dir, new_file_name)\n","        mask_path = os.path.join(mask_output_dir, new_file_name.replace('.jpg', '.png').replace('.JPG', '.png'))\n","\n","        if skip_existing and os.path.isfile(img_path) and os.path.isfile(mask_path):\n","            skipped_count += 1\n","            bar_size = 30\n","            x = int(bar_size * i / nr_images)\n","            sys.stdout.write(\"%s[%s%s] - %i/%i (skipped: %i)\\r\" % ('Processing: ', \"=\" * x, \".\" * (bar_size - x), i + 1, nr_images, skipped_count))\n","            sys.stdout.flush()\n","            continue\n","\n","        try:\n","            if not os.path.isfile(img_path):\n","                response = requests.get(url_original)\n","                response.raise_for_status()\n","\n","                img_raw = Image.open(BytesIO(response.content)).convert('RGB')\n","                rotation_angle = get_exif_rotation_angle(img_raw)\n","\n","                img = ImageOps.exif_transpose(img_raw)\n","                img.save(img_path)\n","                downloaded_count += 1\n","            else:\n","                img_raw = Image.open(img_path).convert('RGB')\n","                rotation_angle = get_exif_rotation_angle(img_raw)\n","                img = ImageOps.exif_transpose(img_raw)\n","\n","            if not skip_existing or not os.path.isfile(mask_path):\n","                if rotation_angle in [90, 270]:\n","                    final_width, final_height = height, width\n","                else:\n","                    final_width, final_height = width, height\n","\n","                mask = np.zeros((final_height, final_width), dtype=np.uint8)\n","                anns = image_id_to_anns[image['id']]\n","\n","                for ann in anns:\n","                    category_id = ann['category_id']\n","                    category_name = category_id_to_name.get(category_id)\n","                    label_id = class_finals.get(category_name, 0)\n","\n","                    segmentation = ann.get('segmentation', [])\n","                    for poly in segmentation:\n","                        if rotation_angle != 0:\n","                            rotated_poly, mask_width, mask_height = apply_rotation_to_polygon(\n","                                poly, rotation_angle, width, height\n","                            )\n","                        else:\n","                            rotated_poly = poly\n","                            mask_width, mask_height = width, height\n","\n","                        mask_img = Image.new('L', (mask_width, mask_height), 0)\n","                        ImageDraw.Draw(mask_img).polygon(rotated_poly, outline=label_id, fill=label_id)\n","                        mask_array = np.array(mask_img)\n","                        mask = np.maximum(mask, mask_array)\n","\n","                img_width, img_height = img.size\n","                if mask.shape[1] != img_width or mask.shape[0] != img_height:\n","                    mask_resized = Image.fromarray(mask).resize((img_width, img_height), Image.NEAREST)\n","                else:\n","                    mask_resized = Image.fromarray(mask)\n","\n","                mask_resized.save(mask_path)\n","\n","        except requests.exceptions.HTTPError as e:\n","            if e.response.status_code == 403:\n","                print(f\"\\nError 403: {file_name} - {url_original}\")\n","            else:\n","                print(f\"\\nErrore HTTP su {file_name}: {e}\")\n","            continue\n","        except Exception as e:\n","            print(f\"\\nErrore nel processare {file_name}: {e}\")\n","            continue\n","\n","        bar_size = 30\n","        x = int(bar_size * i / nr_images)\n","        sys.stdout.write(\"%s[%s%s] - %i/%i (skipped: %i)\\r\" % ('Processing: ', \"=\" * x, \".\" * (bar_size - x), i + 1, nr_images, skipped_count))\n","        sys.stdout.flush()\n","\n","    sys.stdout.write('Finished\\n')\n","\n","    if target_class is not None:\n","        print(f'Scaricate {downloaded_count} nuove immagini della classe {target_class} (saltate {skipped_count}) e generate maschere in {mask_output_dir}')\n","    else:\n","        print(f'Scaricate {downloaded_count} nuove immagini (saltate {skipped_count}) e generate maschere in {mask_output_dir}')\n","\n","    return downloaded_count"],"metadata":{"id":"zmginNr0arv6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["download_taco_images_and_masks(annotations_path='/content/drive/MyDrive/TACO dataset/annotations.json',\n","                               image_output_dir='/content/drive/MyDrive/TACO dataset/images_new_categories',\n","                               mask_output_dir='/content/drive/MyDrive/TACO dataset/masks_new_categories')"],"metadata":{"collapsed":true,"id":"31f2CsmsJZeA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["download_taco_images_and_masks(annotations_path='/content/drive/MyDrive/TACO dataset/annotations_unofficial.json',\n","                               target_class=3,\n","                               image_output_dir='/content/drive/MyDrive/TACO dataset/images_new_categories',\n","                               mask_output_dir='/content/drive/MyDrive/TACO dataset/masks_new_categories')"],"metadata":{"id":"KSoWRqTNz7QV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Rimozione immagini e maschere contenenti la classe 8 (da rimuovere)"],"metadata":{"id":"Y0jBs_bPDHgq"}},{"cell_type":"code","source":["import os\n","from PIL import Image\n","import numpy as np\n","\n","def remove_images_with_class_8(image_dir, mask_dir):\n","    \"\"\"\n","    Elimina le immagini e le maschere se la classe 8 è presente nella maschera (classe da rimuovere).\n","\n","    Args:\n","        image_dir (str): Path alla cartella contenente le immagini.\n","        mask_dir (str): Path alla cartella contenente le maschere (grayscale PNG).\n","    \"\"\"\n","\n","    removed_count = 0\n","    total_files = len(os.listdir(mask_dir))\n","\n","    for i, mask_filename in enumerate(os.listdir(mask_dir)):\n","        if not mask_filename.lower().endswith('.png'):\n","            continue\n","\n","        mask_path = os.path.join(mask_dir, mask_filename)\n","        image_path = os.path.join(image_dir, mask_filename.replace('.png', '.jpg'))\n","\n","        if not os.path.isfile(image_path):\n","            image_path = image_path.replace('.jpg', '.JPG')\n","            if not os.path.isfile(image_path):\n","                print(f\"Immagine non trovata per {mask_filename}\")\n","                continue\n","\n","        try:\n","            mask = np.array(Image.open(mask_path))\n","            if 8 in mask:\n","                os.remove(mask_path)\n","                os.remove(image_path)\n","                removed_count += 1\n","        except Exception as e:\n","            print(f\"Errore con {mask_filename}: {e}\")\n","\n","        bar_size = 30\n","        x = int(bar_size * i / total_files)\n","        print(\"%s[%s%s] - %i/%i\" % ('Elaborazione: ', \"=\" * x, \".\" * (bar_size - x), i+1, total_files), end=\"\\r\")\n","\n","    print(f\"\\nRimosse {removed_count} coppie immagine/maschera contenenti la classe 8.\")\n"],"metadata":{"id":"RSHW1YT9ullQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["remove_images_with_class_8(\"/content/drive/MyDrive/TACO dataset/images_new_categories\", \"/content/drive/MyDrive/TACO dataset/masks_new_categories\")"],"metadata":{"id":"b8ujRNTcumvC","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Elaborazione annotazioni manuali"],"metadata":{"id":"S5tNQJJxGwPd"}},{"cell_type":"code","source":["'''fine_tune_dict = {\n","    0: (0,0,0), # background\n","    1: (102,255,102), #ferro e alluminio\n","    2: (170, 240, 209), # Bottiglie plastica\n","    3: (250,50,83), # Bottiglie vetro\n","    4: (50,183,250), # Tappi e coperchi\n","    5: (221,255,51), # Vetro rotto\n","    6: (51,221,255), # Lattine\n","    7: (89,134, 179), # cartone\n","    8: (36,179,83), # Bicchieri\n","    9: (255,0,204), # Plastica generica\n","    10: (52,209,183), # Carta\n","    11: (250,125,187), # Sacchetti e buste di plastica\n","    12: (34,25,77), # Spazzatura generica\n","    13: (138,138,138), # Cannucce\n","    14: (255,96,55), # Polistirolo\n","    15: (184,61,245) # Sigarette\n","}'''\n","\n","fine_tune_dict={\n","    0: (0,0,0),\n","    1: (51,221,255),\n","    2: (250,50,83),\n","    3: (52,209,183),\n","    4: (255,0,124),\n","    100: (255,96,55),\n","    5: (221,255,51),\n","    6: (36,179,83),\n","    7: (184,61,245)\n","}"],"metadata":{"id":"1MGnyXiRekyh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["creazione maschere RGB -> grayscale"],"metadata":{"id":"Bt33M1cSG-sP"}},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","from tqdm import tqdm\n","\n","def replace_colors_with_grayscale(image_folder, fine_tune_dict, output_folder):\n","    \"\"\"\n","    Sostituisce i colori in un set di immagini con i valori delle chiavi del dizionario in scala di grigi.\n","    \"\"\"\n","    os.makedirs(output_folder, exist_ok=True)\n","\n","    # Creazione della mappa colori\n","    color_map = {tuple(color): key for key, color in fine_tune_dict.items()}\n","\n","    for filename in tqdm(os.listdir(image_folder)):\n","        img_path = os.path.join(image_folder, filename)\n","        img = cv2.imread(img_path)\n","        if img is None:\n","            continue\n","\n","        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        gray_img = np.zeros(img_rgb.shape[:2], dtype=np.uint8)\n","\n","        for rgb_color, gray_value in color_map.items():\n","            mask = np.all(img_rgb == rgb_color, axis=-1)\n","            gray_img[mask] = gray_value\n","\n","        output_path = os.path.join(output_folder, filename)\n","        cv2.imwrite(output_path, gray_img)\n","\n","    print(f\"Processing completed. Grayscale images saved in {output_folder}\")\n","\n","# Esempio di utilizzo\n","image_folder = \"/content/drive/MyDrive/TACO dataset/defaultannot\"\n","output_folder = \"/content/drive/MyDrive/TACO dataset/masks_new_categories\"\n","\n","replace_colors_with_grayscale(image_folder, fine_tune_dict, output_folder)\n"],"metadata":{"id":"HJ5WqCHGhbFD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","\n","def replace_pixel_values(folder_path, old_value, new_value):\n","    \"\"\"\n","    Sostituisce tutti i pixel con valore old_value con new_value\n","    in tutte le immagini grayscale di una cartella.\n","\n","    Le immagini originali vengono sovrascritte.\n","\n","    Args:\n","        folder_path (str): path alla cartella contenente le immagini.\n","        old_value (int): valore pixel da sostituire.\n","        new_value (int): nuovo valore pixel.\n","    \"\"\"\n","    # Controlla che i valori siano validi per immagini 8-bit\n","    if not (0 <= old_value <= 255 and 0 <= new_value <= 255):\n","        raise ValueError(\"old_value e new_value devono essere tra 0 e 255\")\n","\n","    for filename in os.listdir(folder_path):\n","        file_path = os.path.join(folder_path, filename)\n","\n","        # Controlla estensioni comuni di immagini\n","        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')):\n","            # Leggi l'immagine in grayscale\n","            img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n","            if img is None:\n","                continue  # Se non riesce a leggere, passa oltre\n","\n","            # Sostituisci i pixel con valore old_value\n","            img[img == old_value] = new_value\n","\n","            # Sovrascrivi l'immagine\n","            cv2.imwrite(file_path, img)\n","\n","    print(\"Sostituzione completata.\")\n","\n","replace_pixel_values(\n","    folder_path=\"/content/drive/MyDrive/TACO dataset/bibu\",\n","    old_value=2,   # valore da sostituire\n","    new_value=6    # nuovo valore\n",")"],"metadata":{"id":"12X2z1n1u9PL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import Image\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","x = \"/content/drive/MyDrive/TACO dataset/masks_new_categories/7e2e4c62f0712d78_jpg.rf.eb8e4d1c5ad572cb57a8d17d1aa25858.png\"\n","mask_image = Image.open(x).convert('L')\n","mask_image = mask_image.resize((256,256), Image.NEAREST)\n","\n","plt.imshow(mask_image)\n","mask_array = np.array(mask_image)\n","print(np.unique(mask_array))"],"metadata":{"id":"Dw5iDuDaFzEq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Analisi dataset"],"metadata":{"id":"pLI3XqCU7Skm"}},{"cell_type":"markdown","source":["Check coppie immagini-maschera spaiati"],"metadata":{"id":"BYeQ05xKEgxz"}},{"cell_type":"code","source":["import os\n","\n","folder_images = \"/content/drive/MyDrive/TACO dataset/images_new_categories\"\n","folder_masks = \"/content/drive/MyDrive/TACO dataset/masks_new_categories\"\n","\n","images = {os.path.splitext(f)[0] for f in os.listdir(folder_images) }\n","masks = {os.path.splitext(f)[0] for f in os.listdir(folder_masks) }\n","\n","images_senza_maschere = images - masks\n","maschere_senza_immagini = masks - images\n","\n","print(\"Immagini senza maschere:\", images_senza_maschere)\n","print(\"Maschere senza immagini:\", maschere_senza_immagini)\n"],"metadata":{"id":"7wU2ufd4W8iD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["conteggio immagini"],"metadata":{"id":"AHvUBqwlEoFA"}},{"cell_type":"code","source":["import os\n","\n","def count_files_in_folder(folder_path):\n","    return sum(1 for entry in os.scandir(folder_path) if entry.is_file())\n","\n","folder = \"/content/drive/MyDrive/TACO dataset/images_new_categories\"\n","num_files = count_files_in_folder(folder)\n","print(f\"Numero di file: {num_files}\")"],"metadata":{"id":"qgeru-6omM0E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Esempi"],"metadata":{"id":"8avLJ2N5vDmG"}},{"cell_type":"markdown","source":["Esempi di coppie immagine-maschera"],"metadata":{"id":"xnjXqjIcFAKV"}},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Percorsi\n","img_folder = ''\n","masks_folder = ''\n","\n","# Classi mappate (solo le principali con ID > 0)\n","class_labels = {\n","    0: 'Background',\n","    1: 'PLASTICA E POLIMERI',\n","    2: 'METALLI',\n","    3: 'VETRO',\n","    4: 'CARTA E CARTONE',\n","    5: 'POLISTIROLO',\n","    6: 'SIGARETTE',\n","    7: 'NON CLASSIFICATI',\n","}\n","\n","# Colori associati (matplotlib tab20)\n","colors = plt.cm.get_cmap('tab20', 20)\n","\n","def show_image_and_mask(image_path, mask_path):\n","    img = cv2.imread(image_path)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n","\n","    # Colora la maschera\n","    mask_rgb = np.zeros_like(img)\n","    for class_id, label in class_labels.items():\n","        mask_rgb[mask == class_id] = (np.array(colors(class_id))[:3] * 255).astype(np.uint8)\n","\n","    # Estrai classi presenti\n","    present_ids = np.unique(mask)\n","    present_classes = [class_labels[class_id] for class_id in present_ids if class_id in class_labels]\n","\n","    # Plot affiancato\n","    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n","    axs[0].imshow(img)\n","    axs[0].set_title(\"Immagine originale\")\n","    axs[0].axis('off')\n","\n","    axs[1].imshow(mask_rgb)\n","    axs[1].set_title(\"Maschera semantica\")\n","    axs[1].axis('off')\n","\n","    # Legenda\n","    patches = [plt.plot([],[], marker=\"s\", ls=\"\", mec=None,\n","                        color=colors(i)[:3], label=label)[0]\n","               for i, label in class_labels.items() if i in present_ids]\n","    plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc='upper left')\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Stampa classi trovate\n","    print(\"Classi presenti nella maschera:\")\n","    for class_id in present_ids:\n","        if class_id in class_labels:\n","            print(f\"  {class_id}: {class_labels[class_id]}\")\n","    print(\"-\" * 50)\n","\n","\n","N = 1500               # N immagini da visualizzare\n","start_count = 0\n","filename_prefix = \"\"\n","\n","count = 0\n","shown = 0\n","for fname in sorted(os.listdir(img_folder)):\n","    if not fname.endswith('.jpg'):\n","        continue\n","\n","    if filename_prefix and not fname.startswith(filename_prefix):\n","        continue\n","\n","    name, _ = os.path.splitext(fname)\n","    mask_path = os.path.join(masks_folder, f\"{name}.png\")\n","    img_path = os.path.join(img_folder, fname)\n","\n","    if os.path.exists(mask_path):\n","        if count >= start_count:\n","            print(f\"Mostrando: {fname}, count {count}\")\n","            show_image_and_mask(img_path, mask_path)\n","            shown += 1\n","            if shown >= N:\n","                break\n","        count += 1\n","\n","if filename_prefix:\n","    print(f\"\\nFiltro applicato: filename che iniziano con '{filename_prefix}'\")\n","print(f\"Immagini mostrate: {shown}\")\n","print(f\"Immagini totali processate: {count}\")"],"metadata":{"id":"6B8yNyjAsEUC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Creazione split"],"metadata":{"id":"Lgid7NhLGz30"}},{"cell_type":"code","source":["import os\n","import shutil\n","import random\n","from collections import defaultdict\n","from PIL import Image\n","import numpy as np\n","\n","def stratified_fixed_split(mask_folder, image_folder, dest_folder, train_size=1832, val_size=350, test_size=350):\n","    \"\"\"\n","    Esegue lo split del dataset con numero fisso di immagini per split,\n","    mantenendo la distribuzione di classi più bilanciata possibile.\n","\n","    Args:\n","        mask_folder (str): Percorso delle maschere.\n","        image_folder (str): Percorso delle immagini.\n","        dest_folder (str): Cartella di destinazione degli split.\n","        train_size (int): Numero di immagini per il training set.\n","        val_size (int): Numero di immagini per il validation set.\n","        test_size (int): Numero di immagini per il test set.\n","    \"\"\"\n","    os.makedirs(dest_folder, exist_ok=True)\n","    for split in ['train', 'val', 'test']:\n","        os.makedirs(os.path.join(dest_folder, split, 'images'), exist_ok=True)\n","        os.makedirs(os.path.join(dest_folder, split, 'masks'), exist_ok=True)\n","\n","    class_to_files = defaultdict(list)\n","    all_files = sorted([f for f in os.listdir(mask_folder) if f.lower().endswith('.png')])\n","\n","    for fname in all_files:\n","        mask_path = os.path.join(mask_folder, fname)\n","        mask = np.array(Image.open(mask_path))\n","        unique_classes = set(np.unique(mask))\n","        unique_classes.discard(0)\n","        for cls in unique_classes:\n","            class_to_files[cls].append(fname)\n","\n","    unique_files = list(set(f for files in class_to_files.values() for f in files))\n","    random.shuffle(unique_files)\n","\n","    total_required = train_size + val_size + test_size\n","    if total_required > len(unique_files):\n","        raise ValueError(f\"Richiesti {total_required} file ma disponibili solo {len(unique_files)}.\")\n","\n","    selected_files = unique_files[:total_required]\n","    train_files = selected_files[:train_size]\n","    val_files = selected_files[train_size:train_size + val_size]\n","    test_files = selected_files[train_size + val_size:]\n","\n","    def copy_files(file_list, split_name):\n","        for fname in file_list:\n","            src_img = os.path.join(image_folder, fname.replace('.png', '.jpg'))\n","            if not os.path.exists(src_img):\n","                src_img = os.path.join(image_folder, fname.replace('.png', '.JPG'))\n","            dst_img = os.path.join(dest_folder, split_name, 'images', os.path.basename(src_img))\n","            shutil.copy2(src_img, dst_img)\n","\n","            src_mask = os.path.join(mask_folder, fname)\n","            dst_mask = os.path.join(dest_folder, split_name, 'masks', fname)\n","            shutil.copy2(src_mask, dst_mask)\n","\n","    print(\"Copia file...\")\n","    copy_files(train_files, 'train')\n","    copy_files(val_files, 'val')\n","    copy_files(test_files, 'test')\n","\n","    print(f\"Split completato:\")\n","    print(f\"Train: {len(train_files)} immagini\")\n","    print(f\"Val: {len(val_files)} immagini\")\n","    print(f\"Test: {len(test_files)} immagini\")\n","\n","stratified_fixed_split(\n","    mask_folder='/content/drive/MyDrive/TACO dataset/masks_new_categories',\n","    image_folder='/content/drive/MyDrive/TACO dataset/images_new_categories',\n","    dest_folder='/content/drive/MyDrive/TACO dataset/dataset_split',\n","    train_size=2700,\n","    val_size=480,\n","    test_size=0\n",")"],"metadata":{"id":"yAviAdIdR-EY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Analisi Dataloader"],"metadata":{"id":"XX5-puDnYuyw"}},{"cell_type":"code","source":["import os\n","import random\n","import logging\n","from typing import Tuple\n","from PIL import Image, ExifTags\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms.functional as TF\n","from torchvision.transforms import RandomCrop\n","\n","# Setup logging\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","def load_image_with_exif_correction(path: str, convert_mode=\"RGB\") -> Image.Image:\n","    img = Image.open(path)\n","    try:\n","        for orientation in ExifTags.TAGS.keys():\n","            if ExifTags.TAGS[orientation] == 'Orientation':\n","                break\n","        exif = img._getexif()\n","        if exif is not None:\n","            orientation_value = exif.get(orientation, None)\n","            if orientation_value == 3:\n","                img = img.rotate(180, expand=True)\n","            elif orientation_value == 6:\n","                img = img.rotate(270, expand=True)\n","            elif orientation_value == 8:\n","                img = img.rotate(90, expand=True)\n","    except Exception as e:\n","        logger.warning(f\"EXIF non letto su {path}: {e}\")\n","    return img.convert(convert_mode)\n","\n","\n","class TrashDataset(Dataset):\n","    \"\"\"Dataset base immagine-maschera\"\"\"\n","\n","    def __init__(self,\n","                 image_dir: str,\n","                 mask_dir: str,\n","                 resolution: int,\n","                 split: str = \"train\"):\n","        super().__init__()\n","\n","        if not os.path.exists(image_dir):\n","            raise FileNotFoundError(f\"Directory immagini non trovata: {image_dir}\")\n","        if not os.path.exists(mask_dir):\n","            raise FileNotFoundError(f\"Directory maschere non trovata: {mask_dir}\")\n","        if split not in [\"train\", \"val\", \"test\"]:\n","            raise ValueError(\"split deve essere 'train', 'val' o 'test'\")\n","\n","        self.image_dir = image_dir\n","        self.mask_dir = mask_dir\n","        self.resolution = resolution\n","        self.split = split\n","\n","        self._load_file_paths()\n","\n","        logger.info(f\"Dataset '{split}' inizializzato con {len(self)} campioni\")\n","        logger.info(f\"Risoluzione: {resolution}x{resolution}\")\n","\n","    def _load_file_paths(self):\n","        all_images = sorted([f for f in os.listdir(self.image_dir)\n","                             if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n","        all_masks = sorted([f for f in os.listdir(self.mask_dir)\n","                            if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n","\n","        if len(all_images) != len(all_masks):\n","            raise ValueError(f\"Mismatch tra immagini ({len(all_images)}) e maschere ({len(all_masks)})!\")\n","\n","        self.image_paths = all_images\n","        self.mask_paths = all_masks\n","\n","    def __len__(self) -> int:\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n","        image_path = os.path.join(self.image_dir, self.image_paths[idx])\n","        mask_path = os.path.join(self.mask_dir, self.mask_paths[idx])\n","\n","        image = load_image_with_exif_correction(image_path, convert_mode=\"RGB\")\n","        mask = load_image_with_exif_correction(mask_path, convert_mode=\"L\")\n","\n","        image = image.resize((self.resolution, self.resolution), Image.BICUBIC)\n","        mask = mask.resize((self.resolution, self.resolution), Image.NEAREST)\n","\n","        # Analizzo le classi presenti nella maschera\n","        mask_np = np.array(mask)\n","        unique_classes = [c for c in np.unique(mask_np) if c != 0]\n","\n","        # Augmentation SOLO per immagini single-class diverse dalla classe 1\n","        if self.split == \"train\" and len(unique_classes) == 1: #and unique_classes[0] != 1\n","            # Flip orizzontale casuale\n","            if random.random() > 0.5:\n","                image = TF.hflip(image)\n","                mask = TF.hflip(mask)\n","\n","            '''# Flip verticale casuale\n","            if random.random() > 0.5:\n","                image = TF.vflip(image)\n","                mask = TF.vflip(mask)'''\n","\n","            # Rotazioni multiple di 90°\n","            if random.random() > 0.5:\n","                angle = random.choice([90,270])\n","                image = TF.rotate(image, angle)\n","                mask = TF.rotate(mask, angle)\n","\n","            # Traslazioni / crop casuale\n","            if random.random() > 0.5:\n","                i, j, h, w = RandomCrop.get_params(image, output_size=(self.resolution, self.resolution))\n","                image = TF.crop(image, i, j, h, w)\n","                mask = TF.crop(mask, i, j, h, w)\n","\n","            # Variazioni di luminosità e contrasto (solo immagine)\n","            if random.random() > 0.5:\n","                factor = 0.8 + random.random() * 0.4\n","                image = TF.adjust_brightness(image, factor)\n","            if random.random() > 0.5:\n","                factor = 0.8 + random.random() * 0.4\n","                image = TF.adjust_contrast(image, factor)\n","\n","        image_tensor = self._pil_to_tensor_normalized(image)\n","        mask_tensor = self._pil_mask_to_tensor(mask)\n","\n","        return image_tensor, mask_tensor\n","\n","    def _pil_to_tensor_normalized(self, image: Image.Image) -> torch.Tensor:\n","        tensor = TF.to_tensor(image)\n","        return tensor * 2.0 - 1.0  # Normalizzazione in [-1, 1]\n","\n","    def _pil_mask_to_tensor(self, mask: Image.Image) -> torch.Tensor:\n","        return torch.from_numpy(np.array(mask)).long().unsqueeze(0)\n","\n","\n","class BalancedTrashDataset(TrashDataset):\n","    \"\"\"\n","    Bilanciamento con oversampling SOLO su immagini single-class (eccetto classe 1).\n","    Le immagini multi-class vengono usate una sola volta senza oversampling.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 image_dir: str,\n","                 mask_dir: str,\n","                 resolution: int,\n","                 split: str = \"train\",\n","                 target_per_class: int = 1000):\n","\n","        self.balanced_indices = None\n","        super().__init__(image_dir, mask_dir, resolution, split)\n","\n","        if split == \"train\":\n","            logger.info(\"Calcolo distribuzione classi con oversampling mirato (single-class only, no class 1)...\")\n","            self.balanced_indices = self._balance_classes_single_class(target_per_class)\n","            logger.info(f\"Dopo bilanciamento mirato: {len(self.balanced_indices)} campioni\")\n","\n","    def _balance_classes_single_class(self, target_per_class: int):\n","        class_to_indices = {i: [] for i in range(1, 8)}\n","        multi_class_indices = []\n","\n","        # Classificazione immagini in single-class o multi-class\n","        for idx, mask_file in enumerate(self.mask_paths):\n","            mask_path = os.path.join(self.mask_dir, mask_file)\n","            mask = np.array(Image.open(mask_path).convert(\"L\"))\n","            unique_classes = [c for c in np.unique(mask) if c != 0]\n","\n","            if len(unique_classes) == 1:\n","                cls = unique_classes[0]\n","                class_to_indices[cls].append(idx)\n","            elif len(unique_classes) > 1:\n","                multi_class_indices.append(idx)\n","\n","        final_indices = []\n","\n","        # Oversampling SOLO con single-class diverse da 1\n","        for cls, indices in class_to_indices.items():\n","            if cls == 1:  # niente oversampling per la classe 1\n","                final_indices.extend(indices)\n","                continue\n","\n","            if len(indices) >= target_per_class:\n","                chosen = random.sample(indices, target_per_class)\n","            else:\n","                extra_needed = target_per_class - len(indices)\n","                if len(indices) > 0:\n","                    extra = np.random.choice(indices, extra_needed, replace=True).tolist()\n","                    chosen = indices + extra\n","                else:\n","                    chosen = []\n","            final_indices.extend(chosen)\n","\n","        # Aggiungi le immagini multi-classe una sola volta\n","        final_indices.extend(multi_class_indices)\n","\n","        random.shuffle(final_indices)\n","        return final_indices\n","\n","    def __len__(self):\n","        if self.split == \"train\" and self.balanced_indices is not None:\n","            return len(self.balanced_indices)\n","        return super().__len__()\n","\n","    def __getitem__(self, idx: int):\n","        if self.split == \"train\" and self.balanced_indices is not None:\n","            idx = self.balanced_indices[idx]\n","        return super().__getitem__(idx)\n","\n","\n","def load_data(image_path: str,\n","              mask_path: str,\n","              batch_size: int,\n","              image_size: int,\n","              split: str,\n","              num_workers: int = 2,\n","              pin_memory: bool = True,\n","              persistent_workers: bool = True,\n","              target_per_class: int = 1000) -> DataLoader:\n","\n","    if split == \"train\":\n","        dataset = BalancedTrashDataset(\n","            image_dir=image_path,\n","            mask_dir=mask_path,\n","            resolution=image_size,\n","            split=split,\n","            target_per_class=target_per_class\n","        )\n","    else:\n","        dataset = TrashDataset(\n","            image_dir=image_path,\n","            mask_dir=mask_path,\n","            resolution=image_size,\n","            split=split\n","        )\n","\n","    shuffle = True if split == \"train\" else False\n","\n","    loader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        num_workers=num_workers,\n","        pin_memory=pin_memory,\n","        drop_last=True,\n","        persistent_workers=persistent_workers if num_workers > 0 else False\n","    )\n","\n","    print(f\"DataLoader '{split}' creato: {len(dataset)} campioni, \"\n","          f\"batch_size={batch_size}, num_workers={num_workers}\")\n","\n","    return loader"],"metadata":{"id":"2e5sZfckZAFH","executionInfo":{"status":"ok","timestamp":1757927646214,"user_tz":-120,"elapsed":50,"user":{"displayName":"davide","userId":"08345280063963440379"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["\n","train_loader = load_data(\n","    image_path=\"/content/drive/MyDrive/TACO dataset/dataset_split/train/images\",\n","    mask_path=\"/content/drive/MyDrive/TACO dataset/dataset_split/train/masks\",\n","    batch_size=50,\n","    image_size=512,\n","    split=\"train\",\n","    target_per_class=850\n",")"],"metadata":{"id":"HUdPelJFZMR6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","def count_class_occurrences(dataset):\n","    \"\"\"\n","    Conta in quante immagini appare ogni classe nel dataset.\n","\n","    Args:\n","        dataset: TrashDataset o BalancedTrashDataset\n","\n","    Returns:\n","        dict: chiave = classe, valore = numero di immagini in cui appare\n","    \"\"\"\n","    class_counts = {i: 0 for i in range(1, 8)}  # assumiamo classi 1-7\n","\n","    for idx in range(len(dataset)):\n","        # Prendi solo la maschera\n","        _, mask = dataset[idx]\n","        mask = mask.squeeze().numpy()  # [H,W]\n","\n","        unique_classes = np.unique(mask)\n","        unique_classes = unique_classes[unique_classes != 0]  # ignora background\n","\n","        for c in unique_classes:\n","            class_counts[c] += 1\n","\n","    return class_counts\n","\n","class_distribution = count_class_occurrences(train_loader.dataset)\n","\n","print(\"Numero di immagini per classe:\")\n","for cls, count in class_distribution.items():\n","    print(f\"Classe {cls}: {count} immagini\")"],"metadata":{"id":"hXqJD_F9dNSy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Conteggio occorrenze classi"],"metadata":{"id":"JGTKkOX6u65y"}},{"cell_type":"code","source":["import os\n","from collections import defaultdict\n","from PIL import Image\n","import numpy as np\n","\n","def count_class_occurrences_in_masks(mask_folder):\n","    \"\"\"\n","    Conta quante volte ciascun valore di classe appare almeno una volta in ogni maschera.\n","\n","    Args:\n","        mask_folder (str): Cartella contenente le maschere semantiche (.png)\n","\n","    Returns:\n","        dict: classe -> numero di immagini in cui appare\n","    \"\"\"\n","    class_counts = defaultdict(int)\n","\n","    mask_files = [f for f in os.listdir(mask_folder) if f.lower().endswith(\".png\")]\n","\n","    for mask_file in mask_files:\n","        mask_path = os.path.join(mask_folder, mask_file)\n","        mask = np.array(Image.open(mask_path))\n","\n","        # Estrai i valori unici presenti nella maschera\n","        unique_values = np.unique(mask)\n","\n","        for val in unique_values:\n","            class_counts[int(val)] += 1\n","\n","    return dict(class_counts)"],"metadata":{"id":"2JuQR3_CXUhI","executionInfo":{"status":"ok","timestamp":1757924334806,"user_tz":-120,"elapsed":55,"user":{"displayName":"davide","userId":"08345280063963440379"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["mask_dir = \"/content/drive/MyDrive/TACO dataset/masks_new_categories\"\n","counts = count_class_occurrences_in_masks(mask_dir)\n","\n","# Stampa ordinata\n","for class_id in sorted(counts):\n","    print(f\"Classe {class_id}: presente in {counts[class_id]} maschere\")\n","\n","'''\n","    0: 'Background',\n","    1: 'PLASTICA E POLIMERI',\n","    2: 'METALLI',\n","    3: 'VETRO',\n","    4: 'CARTA E CARTONE',\n","    5: 'POLISTIROLO',\n","    6: 'SIGARETTE',\n","    7: 'NON CLASSIFICATI'\n","'''"],"metadata":{"id":"yOGdnn7gXX7Y"},"execution_count":null,"outputs":[]}]}